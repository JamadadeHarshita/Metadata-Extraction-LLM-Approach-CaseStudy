{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35d49b97-55a8-40f7-9861-f450ab0cb16b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in ./.local/lib/python3.11/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.66.5)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24bf0abf-f7bd-4f31-abe7-9350fce54151",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import transformers\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6436c012-5008-435b-89b5-658b701e9855",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_transcript(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \n",
    "    # Ensure 'Transkript' column has no NaN values\n",
    "    data['Transkript'] = data['Transkript'].fillna(\"\").astype(str)\n",
    "\n",
    "    # Chunking logic\n",
    "    chunked_data = []\n",
    "    current_speaker = None\n",
    "    current_text = \"\"\n",
    "    initial_timestamp = \"\"\n",
    "\n",
    "    for index, row in data.iterrows():\n",
    "        speaker = row['Sprecher']\n",
    "        transcript = row['Transkript']\n",
    "        timestamp = row['Timecode'] \n",
    "\n",
    "        if speaker != current_speaker:\n",
    "            # Save previous chunk if exists\n",
    "            if current_speaker is not None:\n",
    "                chunked_data.append({\n",
    "                    'Speaker': current_speaker,\n",
    "                    'Transcript': current_text.strip(),\n",
    "                    'Initial_Timestamp' : initial_timestamp,\n",
    "                    'Current_Timestamp' : timestamp\n",
    "                })\n",
    "            \n",
    "            # Start a new chunk\n",
    "            current_speaker = speaker\n",
    "            current_text = transcript\n",
    "            initial_timestamp = timestamp \n",
    "        else:\n",
    "            # Continue appending to the same speaker's chunk\n",
    "            current_text += \" \" + transcript if isinstance(transcript, str) else \"\"\n",
    "\n",
    "    # Save the last chunk\n",
    "    if current_speaker is not None:\n",
    "        chunked_data.append({\n",
    "            'Speaker': current_speaker,\n",
    "            'Transcript': current_text.strip(),\n",
    "            'Initial_Timestamp' : initial_timestamp,\n",
    "            'Current_Timestamp' : timestamp\n",
    "        })\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    chunked_df = pd.DataFrame(chunked_data)\n",
    "    \n",
    "    return chunked_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc851cb5-4772-4693-a60f-fe27baf3425b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_by_sentence(chunked_df: pd.DataFrame, min_tokens=256, max_tokens=512) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Further splits chunks by sentence while ensuring each chunk is within a token range.\n",
    "\n",
    "    Args:\n",
    "        chunked_df (pd.DataFrame): Input DataFrame with 'Speaker' and 'Transcript' columns.\n",
    "        min_tokens (int): Minimum number of tokens per chunk.\n",
    "        max_tokens (int): Maximum number of tokens per chunk.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with sentence-based chunked transcripts.\n",
    "    \"\"\"\n",
    "\n",
    "    # Function to count tokens (approximate, assuming 1 word ≈ 1.2 tokens)\n",
    "    def count_tokens(text):\n",
    "        return len(text.split()) * 1.2  # Rough estimate\n",
    "\n",
    "    # Initialize list for final merged chunks\n",
    "    merged_chunks = []\n",
    "    temp_chunk = []\n",
    "    temp_token_count = 0\n",
    "    speaker = None\n",
    "    initial_timestamp = \"\"\n",
    "    final_timestamp = \"\"\n",
    "\n",
    "    # Process each row\n",
    "    for _, row in chunked_df.iterrows():\n",
    "        sentence = row['Transcript']\n",
    "        sentence_tokens = count_tokens(sentence)\n",
    "\n",
    "        # If adding this chunk keeps us within MAX_TOKENS\n",
    "        if temp_token_count + sentence_tokens <= max_tokens:\n",
    "            if not temp_chunk:\n",
    "                speaker = row['Speaker']  # Store speaker only for new chunks\n",
    "                initial_timestamp = row['Initial_Timestamp']\n",
    "            temp_chunk.append(sentence)\n",
    "            temp_token_count += sentence_tokens\n",
    "        else:\n",
    "            # Save the previous chunk before starting a new one\n",
    "            if temp_chunk:\n",
    "                merged_chunks.append({\n",
    "                    'Speaker': speaker,\n",
    "                    'Transcript': \" \".join(temp_chunk),\n",
    "                    'Initial_Timestamp' : initial_timestamp,\n",
    "                    'Current_Timestamp' : row['Current_Timestamp']\n",
    "                })\n",
    "\n",
    "            # Start a new chunk with the current sentence\n",
    "            temp_chunk = [sentence]\n",
    "            temp_token_count = sentence_tokens\n",
    "            speaker = row['Speaker']\n",
    "            initial_timestamp = row['Initial_Timestamp']\n",
    "            final_timestamp = row['Current_Timestamp']\n",
    "\n",
    "    # Save last chunk if any content remains\n",
    "    if temp_chunk:\n",
    "        merged_chunks.append({\n",
    "            'Speaker': speaker,\n",
    "            'Transcript': \" \".join(temp_chunk),\n",
    "            'Initial_Timestamp' : initial_timestamp,\n",
    "            'Current_Timestamp' : final_timestamp\n",
    "        })\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    final_merged_df = pd.DataFrame(merged_chunks)\n",
    "    \n",
    "    return final_merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5e7d285-cb30-4f3f-b232-debf27d00341",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2827e8447b4344dc996e14a4cb86d3ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_path = \"models/meta-llama/Llama-3.3-70B-Instruct\"\n",
    "quantization_config = transformers.BitsAndBytesConfig(llm_int8_enable_fp32_cpu_offload=True)\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\", torch_dtype=torch.bfloat16, quantization_config=quantization_config)\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_path)\n",
    "nlp_pipeline = pipeline(\"text-generation\", model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5e477f9-da20-4f79-bad0-ef5196ce1321",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.pipelines.text_generation.TextGenerationPipeline at 0x7fdcadc6b4d0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d45450-b828-406f-b791-96dbee91ee33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_metadata(model_name: str, chunks: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extract metadata from multiple chunks of a German transcript using the specified model.\n",
    "    \n",
    "    Args:\n",
    "        client: The Groq client object.\n",
    "        model_name: The name of the model to use (e.g., 'llama3-8b-8192').\n",
    "        chunks: A DataFrame with 'Speaker' and 'Transcript' columns.\n",
    "    \n",
    "    Returns:\n",
    "        A DataFrame containing the extracted metadata for all chunks.\n",
    "    \"\"\"\n",
    "    all_metadata = []\n",
    "    num_chunks = len(chunks)\n",
    "\n",
    "    for i, row in chunks.iloc[:5].iterrows():  # Iterate over DataFrame rows\n",
    "        speaker = row[\"Speaker\"]\n",
    "        transcript = row[\"Transcript\"]\n",
    "        timestamp = row['Timestamp']\n",
    "\n",
    "        print(f\"Processing chunk {i+1}/{len(chunks)} for Speaker: {speaker}...\")\n",
    "\n",
    "        # Prompt for metadata extraction\n",
    "        prompt = f\"\"\"\n",
    "Extrahieren Sie die folgenden Informationen aus dem Transkript und geben Sie die Antwort auf Deutsch ein. \n",
    "Wenn die Antwort nicht gefunden wurde, geben Sie stattdessen %%% zurück. \n",
    "Fügen Sie nach jedem Wert in Klammern den entsprechenden {timestamp} hinzu. \n",
    "aber nur, wenn der Wert nicht %%% ist und den Wert aus dem Kontext behalten, aber den Zeitstempel nicht hinzufügen. Fügen Sie keine Präambel ein.\n",
    "\n",
    "\n",
    "Transkript:\n",
    "{transcript}\n",
    "\n",
    "Metadaten columns to be present:\n",
    "        NAME: \n",
    "        JAHRGANG: \n",
    "        ORT: \n",
    "        GESCHLECHT: \n",
    "        BERUF: \n",
    "        VAT_JG: \n",
    "        VAT_KONFESSION: \n",
    "        VAT_HERKUN: \n",
    "        VAT_SCHULE: \n",
    "        VAT_AUSBIL: \n",
    "        VAT_STAND: \n",
    "        VAT_POLOR:\n",
    "\n",
    "Fügen Sie keine Präambel ein.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "        # Generate response using the Groq client\n",
    "        response = nlp_pipeline(prompt, max_new_tokens=200, truncation=True)[0][\"generated_text\"]\n",
    "\n",
    "        # Extract metadata from the response\n",
    "        metadata = response\n",
    "        # Parse metadata into a dictionary\n",
    "        extracted_metadata = {\"Speaker\": speaker}  # Store speaker info\n",
    "        for line in metadata.split(\"\\n\"):\n",
    "            if \":\" in line:\n",
    "                key, value = line.split(\":\", 1)\n",
    "                extracted_metadata[key.strip()] = value.strip()\n",
    "\n",
    "        # Remove \"%%%%\" values before storing\n",
    "        extracted_metadata = {k: (v if v != \"%%%\" else \"\") for k, v in extracted_metadata.items()}\n",
    "\n",
    "        if i < 5 - 1:  # If not the last iteration\n",
    "            extracted_metadata = {k: v.strip() + \",\" if v else v.strip() for k, v in extracted_metadata.items()}\n",
    "\n",
    "        # Append metadata for this chunk\n",
    "        all_metadata.append(extracted_metadata)\n",
    "\n",
    "    # Convert metadata list into a DataFrame\n",
    "    return pd.DataFrame(all_metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "66dc81a9-7248-402f-aaae-5de035b0b945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing file: adg0001_er_2024_10_31.csv\n",
      "Processing chunk 1/21 for Speaker: INT_AH...\n",
      "Processing chunk 2/21 for Speaker: IP_FA...\n",
      "Processing chunk 3/21 for Speaker: INT_AH...\n",
      "Processing chunk 4/21 for Speaker: IP_FA...\n",
      "Processing chunk 5/21 for Speaker: INT_AH...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "folder_path = \"Transcripts\"\n",
    "MODEL = \"llama-3.3-70b-versatile\"\n",
    "\n",
    "# List to store metadata for all files\n",
    "all_metadata = []\n",
    "for filename in os.listdir(folder_path):\n",
    "    file_path = os.path.join(folder_path, filename)\n",
    "    if os.path.isfile(file_path) & filename.endswith(\".csv\"):\n",
    "        print(f\"\\nProcessing file: {filename}\")\n",
    "        input_data = pd.read_csv(file_path, sep=None, engine='python')\n",
    "        speaker_chunks_df = chunk_transcript(input_data)  # Stores speaker-based chunks\n",
    "        final_chunks_df = chunk_by_sentence(speaker_chunks_df)\n",
    "        final_chunks_df['Timestamp'] = final_chunks_df['Initial_Timestamp'] + \" - \" + final_chunks_df['Current_Timestamp'] \n",
    "        final_chunks_df.drop(columns=['Initial_Timestamp', \"Current_Timestamp\"], inplace=True)\n",
    "        # print(final_chunks_df)\n",
    "        \n",
    "\n",
    "        # Extract metadata for the chunks\n",
    "        llama_70b_responses = extract_metadata(nlp_pipeline, final_chunks_df)\n",
    "        # Ensure that the response DataFrame contains metadata columns\n",
    "        if not llama_70b_responses.empty:\n",
    "            # Merge chunk outputs into a single row \n",
    "            merged_metadata = llama_70b_responses.apply(lambda col: ' '.join(col.dropna().astype(str)))\n",
    "            \n",
    "            for column in merged_metadata.index:\n",
    "                unique_values = set([value.strip() for value in merged_metadata[column].strip().split(\",\")])\n",
    "                list_unique_values = list(filter(None, unique_values))\n",
    "                merged_metadata[column] = \" | \".join(list_unique_values)\n",
    "\n",
    "            # Add filename for reference\n",
    "            # merged_metadata[\"Filename\"] = filename  \n",
    "\n",
    "            # Append to list\n",
    "            all_metadata.append(merged_metadata)\n",
    "        else:\n",
    "            print(f\"No metadata extracted from {filename}\")\n",
    "         \n",
    "        time.sleep(0.5)\n",
    "\n",
    "# Convert list of metadata rows into a single DataFrame\n",
    "final_metadata_df = pd.DataFrame(all_metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a643c5d7-2dbe-406b-8332-b9061b7589b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Speaker</th>\n",
       "      <th>Fügen Sie nach jedem Wert in Klammern den entsprechenden 00</th>\n",
       "      <th>Transkript</th>\n",
       "      <th>Metadaten columns to be present</th>\n",
       "      <th>NAME</th>\n",
       "      <th>JAHRGANG</th>\n",
       "      <th>ORT</th>\n",
       "      <th>GESCHLECHT</th>\n",
       "      <th>BERUF</th>\n",
       "      <th>VAT_JG</th>\n",
       "      <th>VAT_KONFESSION</th>\n",
       "      <th>VAT_HERKUN</th>\n",
       "      <th>VAT_SCHULE</th>\n",
       "      <th>VAT_AUSBIL</th>\n",
       "      <th>VAT_STAND</th>\n",
       "      <th>VAT_POLOR</th>\n",
       "      <th>Der Tagesablauf, ja halt wecken, wie früher im Landjahr auch, um 6 Uhr wurde geweckt, im Winter um 7 Uhr, und dann war normalerweise Frühsport. Entweder sind wir ..., im Sommer haben wir draußen Frühsport gemacht und im Winter im ..., also wir haben schon mal einen Lauf gemacht oder so Freiübungen und im Winter, dann haben wir das halt im Hof nur so Freiübungen gemacht und keine Läufe. Das hat so vielleicht sagen wir mal 10 Minuten bis eine Viertelstunde. Und dann ging’s zum Waschen, in die Waschräume, in die Duschräume und anschließend war Frühstück. Ja nach dem Frühstück wurde dann die Arbeit verteilt. Die ersten 4 Wochen blieb man halt im Lager. Da wurde man vertraut gemacht mit eben all den Dingen, mit den Tageseinteilungen. Und dann wurde eingeteilt zum Hausdienst, also, sagen wir mal, sauber machen oder Waschküchengruppe, Bügelgruppe oder Küche und auch Verwaltungsarbeiten. Also im Büro mussten wir auch helfen. Eben, sagen wir mal, es wurde jeder getestet nach seinen Fähigkeiten und auch mehr oder weniger denn da eingesetzt. Er musste zwar alles lernen, er kam überall mit rein aber, ich meine, viele schimpfen auf den Arbeitsdienst und sagen, das war Mist, das war Blödsinn. Vielleicht lag’s an der Führung aber, wie gesagt, wir hatten das Glück, wir hatten ne wirklich, also sehr gute Führerin, die äußerst korrekt war. Also wirklich äußerst korrekt. Die weder Entgleisungen duldete, noch sich selbst, sagen wir mal, irgendwie gehen ließ. Die auch sah, wenn irgendwelche Ungerechtigkeiten waren. Zum Beispiel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INT_AH | IP_FA</td>\n",
       "      <td>07:22.00 - 00:16:30.00 hinzu. | 00:06.00 - 00:...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>%%% (00:07:22.00 - 00:16:30.00) | %%% (00:04:1...</td>\n",
       "      <td>%%% (00:07:22.00 - 00:16:30.00) | %%% (00:04:1...</td>\n",
       "      <td>%%% (00:04:11.00 - 00:07:23.00) | Mülheim an d...</td>\n",
       "      <td>%%% (00:07:22.00 - 00:16:30.00) | %%% (00:04:1...</td>\n",
       "      <td>Büro (00:04:11.00 - 00:07:23.00) | %%% (00:07:...</td>\n",
       "      <td>42 (00:07:22.00 - 00:16:30.00) | %%% (00:04:11...</td>\n",
       "      <td>%%% (00:07:22.00 - 00:16:30.00) | %%% (00:04:1...</td>\n",
       "      <td>%%% (00:07:22.00 | %%% (00:04:11.00 - 00:07:23...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>ans Bett gebracht | entgleiste die Lokomotive ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Speaker Fügen Sie nach jedem Wert in Klammern den entsprechenden 00  \\\n",
       "0  INT_AH | IP_FA  07:22.00 - 00:16:30.00 hinzu. | 00:06.00 - 00:...            \n",
       "\n",
       "  Transkript Metadaten columns to be present  \\\n",
       "0                                              \n",
       "\n",
       "                                                NAME  \\\n",
       "0  %%% (00:07:22.00 - 00:16:30.00) | %%% (00:04:1...   \n",
       "\n",
       "                                            JAHRGANG  \\\n",
       "0  %%% (00:07:22.00 - 00:16:30.00) | %%% (00:04:1...   \n",
       "\n",
       "                                                 ORT  \\\n",
       "0  %%% (00:04:11.00 - 00:07:23.00) | Mülheim an d...   \n",
       "\n",
       "                                          GESCHLECHT  \\\n",
       "0  %%% (00:07:22.00 - 00:16:30.00) | %%% (00:04:1...   \n",
       "\n",
       "                                               BERUF  \\\n",
       "0  Büro (00:04:11.00 - 00:07:23.00) | %%% (00:07:...   \n",
       "\n",
       "                                              VAT_JG  \\\n",
       "0  42 (00:07:22.00 - 00:16:30.00) | %%% (00:04:11...   \n",
       "\n",
       "                                      VAT_KONFESSION  \\\n",
       "0  %%% (00:07:22.00 - 00:16:30.00) | %%% (00:04:1...   \n",
       "\n",
       "                                          VAT_HERKUN VAT_SCHULE VAT_AUSBIL  \\\n",
       "0  %%% (00:07:22.00 | %%% (00:04:11.00 - 00:07:23...                         \n",
       "\n",
       "  VAT_STAND VAT_POLOR  \\\n",
       "0                       \n",
       "\n",
       "  Der Tagesablauf, ja halt wecken, wie früher im Landjahr auch, um 6 Uhr wurde geweckt, im Winter um 7 Uhr, und dann war normalerweise Frühsport. Entweder sind wir ..., im Sommer haben wir draußen Frühsport gemacht und im Winter im ..., also wir haben schon mal einen Lauf gemacht oder so Freiübungen und im Winter, dann haben wir das halt im Hof nur so Freiübungen gemacht und keine Läufe. Das hat so vielleicht sagen wir mal 10 Minuten bis eine Viertelstunde. Und dann ging’s zum Waschen, in die Waschräume, in die Duschräume und anschließend war Frühstück. Ja nach dem Frühstück wurde dann die Arbeit verteilt. Die ersten 4 Wochen blieb man halt im Lager. Da wurde man vertraut gemacht mit eben all den Dingen, mit den Tageseinteilungen. Und dann wurde eingeteilt zum Hausdienst, also, sagen wir mal, sauber machen oder Waschküchengruppe, Bügelgruppe oder Küche und auch Verwaltungsarbeiten. Also im Büro mussten wir auch helfen. Eben, sagen wir mal, es wurde jeder getestet nach seinen Fähigkeiten und auch mehr oder weniger denn da eingesetzt. Er musste zwar alles lernen, er kam überall mit rein aber, ich meine, viele schimpfen auf den Arbeitsdienst und sagen, das war Mist, das war Blödsinn. Vielleicht lag’s an der Führung aber, wie gesagt, wir hatten das Glück, wir hatten ne wirklich, also sehr gute Führerin, die äußerst korrekt war. Also wirklich äußerst korrekt. Die weder Entgleisungen duldete, noch sich selbst, sagen wir mal, irgendwie gehen ließ. Die auch sah, wenn irgendwelche Ungerechtigkeiten waren. Zum Beispiel  \n",
       "0  ans Bett gebracht | entgleiste die Lokomotive ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_metadata_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3bb53937-9e0b-4fa3-89c9-d6385c8a2d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_metadata_df.to_csv(\"metadata_results.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
