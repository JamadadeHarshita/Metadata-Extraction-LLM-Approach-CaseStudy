{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "35d49b97-55a8-40f7-9861-f450ab0cb16b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: To modify pip, please run the following command:\n",
      "C:\\Users\\anavn\\AppData\\Local\\Programs\\Python\\Python312\\python.exe -m pip install -Uqqq pip --progress-bar off\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Requirement already satisfied: groq in c:\\users\\anavn\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.13.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\anavn\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from groq) (4.8.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\anavn\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\anavn\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from groq) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\anavn\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from groq) (2.10.3)\n",
      "Requirement already satisfied: sniffio in c:\\users\\anavn\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from groq) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in c:\\users\\anavn\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from groq) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\anavn\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\anavn\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (2024.12.14)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\anavn\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\anavn\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\anavn\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in c:\\users\\anavn\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (2.27.1)\n",
      "Requirement already satisfied: nltk in c:\\users\\anavn\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\anavn\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\anavn\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\anavn\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\anavn\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\anavn\\appdata\\roaming\\python\\python312\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install -Uqqq pip --progress-bar off\n",
    "!pip install -qqq groq==0.13.0 --progress-bar off\n",
    "!pip install -qqq python-dotenv==1.0.1 --progress-bar off\n",
    "!pip install groq\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bf0abf-f7bd-4f31-abe7-9350fce54151",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\anavn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6436c012-5008-435b-89b5-658b701e9855",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_transcript(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \n",
    "    # Ensure 'Transkript' column has no NaN values\n",
    "    data['Transkript'] = data['Transkript'].fillna(\"\").astype(str)\n",
    "\n",
    "    # Chunking logic\n",
    "    chunked_data = []\n",
    "    current_speaker = None\n",
    "    current_text = \"\"\n",
    "\n",
    "    for index, row in data.iterrows():\n",
    "        speaker = row['Sprecher']\n",
    "        transcript = row['Transkript']\n",
    "\n",
    "        if speaker != current_speaker:\n",
    "            # Save previous chunk if exists\n",
    "            if current_speaker is not None:\n",
    "                chunked_data.append({\n",
    "                    'Speaker': current_speaker,\n",
    "                    'Transcript': current_text.strip()\n",
    "                })\n",
    "            \n",
    "            # Start a new chunk\n",
    "            current_speaker = speaker\n",
    "            current_text = transcript\n",
    "        else:\n",
    "            # Continue appending to the same speaker's chunk\n",
    "            current_text += \" \" + transcript if isinstance(transcript, str) else \"\"\n",
    "\n",
    "    # Save the last chunk\n",
    "    if current_speaker is not None:\n",
    "        chunked_data.append({\n",
    "            'Speaker': current_speaker,\n",
    "            'Transcript': current_text.strip()\n",
    "        })\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    chunked_df = pd.DataFrame(chunked_data)\n",
    "    \n",
    "    return chunked_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc851cb5-4772-4693-a60f-fe27baf3425b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_by_sentence(chunked_df: pd.DataFrame, min_tokens=256, max_tokens=512) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Further splits chunks by sentence while ensuring each chunk is within a token range.\n",
    "\n",
    "    Args:\n",
    "        chunked_df (pd.DataFrame): Input DataFrame with 'Speaker' and 'Transcript' columns.\n",
    "        min_tokens (int): Minimum number of tokens per chunk.\n",
    "        max_tokens (int): Maximum number of tokens per chunk.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with sentence-based chunked transcripts.\n",
    "    \"\"\"\n",
    "\n",
    "    # Function to count tokens (approximate, assuming 1 word ≈ 1.2 tokens)\n",
    "    def count_tokens(text):\n",
    "        return len(text.split()) * 1.2  # Rough estimate\n",
    "\n",
    "    # Initialize list for final merged chunks\n",
    "    merged_chunks = []\n",
    "    temp_chunk = []\n",
    "    temp_token_count = 0\n",
    "    speaker = None\n",
    "\n",
    "    # Process each row\n",
    "    for _, row in chunked_df.iterrows():\n",
    "        sentence = row['Transcript']\n",
    "        sentence_tokens = count_tokens(sentence)\n",
    "\n",
    "        # If adding this chunk keeps us within MAX_TOKENS\n",
    "        if temp_token_count + sentence_tokens <= max_tokens:\n",
    "            if not temp_chunk:\n",
    "                speaker = row['Speaker']  # Store speaker only for new chunks\n",
    "            temp_chunk.append(sentence)\n",
    "            temp_token_count += sentence_tokens\n",
    "        else:\n",
    "            # Save the previous chunk before starting a new one\n",
    "            if temp_chunk:\n",
    "                merged_chunks.append({\n",
    "                    'Speaker': speaker,\n",
    "                    'Transcript': \" \".join(temp_chunk)\n",
    "                })\n",
    "\n",
    "            # Start a new chunk with the current sentence\n",
    "            temp_chunk = [sentence]\n",
    "            temp_token_count = sentence_tokens\n",
    "            speaker = row['Speaker']\n",
    "\n",
    "    # Save last chunk if any content remains\n",
    "    if temp_chunk:\n",
    "        merged_chunks.append({\n",
    "            'Speaker': speaker,\n",
    "            'Transcript': \" \".join(temp_chunk)\n",
    "        })\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    final_merged_df = pd.DataFrame(merged_chunks)\n",
    "    \n",
    "    return final_merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "e744d3c1-7d68-4abc-bfec-e32b54bed9ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Groq client initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "from groq import Groq\n",
    "import pandas as pd \n",
    "\n",
    "\n",
    "# api_key = \"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\"\n",
    "# api_key = \"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\"\n",
    "# api_key = \"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\"\n",
    "# api_key = \"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\"\n",
    "api_key = \"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\"\n",
    "# Initialize the Groq client\n",
    "client = Groq(api_key=api_key)\n",
    "\n",
    "print(\"Groq client initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "bed53c67-a80b-4447-b24b-ec542bc7e00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_metadata(client, model_name: str, chunks: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extract metadata from multiple chunks of a German transcript using the specified model.\n",
    "    \n",
    "    Args:\n",
    "        client: The Groq client object.\n",
    "        model_name: The name of the model to use (e.g., 'llama3-8b-8192').\n",
    "        chunks: A DataFrame with 'Speaker' and 'Transcript' columns.\n",
    "    \n",
    "    Returns:\n",
    "        A DataFrame containing the extracted metadata for all chunks.\n",
    "    \"\"\"\n",
    "    all_metadata = []\n",
    "    num_chunks = len(chunks)\n",
    "\n",
    "    for i, row in chunks.iterrows():  # Iterate over DataFrame rows\n",
    "        speaker = row[\"Speaker\"]\n",
    "        transcript = row[\"Transcript\"]\n",
    "\n",
    "        print(f\"Processing chunk {i+1}/{len(chunks)} for Speaker: {speaker}...\")\n",
    "\n",
    "        # Prompt for metadata extraction\n",
    "        prompt = f\"\"\"\n",
    "        Extrahieren Sie die folgenden Informationen aus dem Transkript und geben Sie die Antwort auf Deutsch ein. Wenn die Antwort nicht gefunden wurde, geben Sie sie als %%% zurück:\n",
    "\n",
    "        ### Persönliche Informationen:\n",
    "        NAME: Der vollständige Name der Person (Vor- und Nachname, falls verfügbar).\n",
    "        JAHRGANG: Das Geburtsjahr der Person.\n",
    "        ORT: Der Geburtsort der Person.\n",
    "        GESCHLECHT: Das Geschlecht der Person (z.B. männlich, weiblich, divers).\n",
    "        BERUF: Der aktuelle Beruf der Person.\n",
    "\n",
    "        ### Informationen über Vater und Partner:\n",
    "        VAT_JG: Geburtsjahr des Vaters.\n",
    "        VAT_KONFESSION: Religion des Vaters.\n",
    "        VAT_HERKUN: Herkunft des Vaters.\n",
    "        VAT_SCHULE: Schulbildung des Vaters.\n",
    "        VAT_AUSBIL: Ausbildung des Vaters.\n",
    "        VAT_STAND: Beruflicher Status des Vaters.\n",
    "        VAT_POLOR: Politische Orientierung des Vaters.\n",
    "\n",
    "        Transkript:\n",
    "        {transcript}\n",
    "\n",
    "        Metadaten:\n",
    "        NAME:\n",
    "        JAHRGANG:\n",
    "        ORT:\n",
    "        GESCHLECHT:\n",
    "        BERUF:\n",
    "        VAT_JG:\n",
    "        VAT_KONFESSION:\n",
    "        VAT_HERKUN:\n",
    "        VAT_SCHULE:\n",
    "        VAT_AUSBIL:\n",
    "        VAT_STAND:\n",
    "        VAT_POLOR:\n",
    "\n",
    "        Fügen Sie keine Präambel ein.\n",
    "        \"\"\"\n",
    "\n",
    "        # Generate response using the Groq client\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0,\n",
    "        )\n",
    "\n",
    "        # Extract metadata from the response\n",
    "        metadata = response.choices[0].message.content\n",
    "\n",
    "        # Parse metadata into a dictionary\n",
    "        extracted_metadata = {\"Speaker\": speaker}  # Store speaker info\n",
    "        for line in metadata.split(\"\\n\"):\n",
    "            if \":\" in line:\n",
    "                key, value = line.split(\":\", 1)\n",
    "                extracted_metadata[key.strip()] = value.strip()\n",
    "\n",
    "        # Remove \"%%%%\" values before storing\n",
    "        extracted_metadata = {k: (v if v != \"%%%\" else \"\") for k, v in extracted_metadata.items()}\n",
    "\n",
    "        if i < num_chunks - 1:  # If not the last iteration\n",
    "            extracted_metadata = {k: v.strip() + \",\" if v else v.strip() for k, v in extracted_metadata.items()}\n",
    "\n",
    "        # Append metadata for this chunk\n",
    "        all_metadata.append(extracted_metadata)\n",
    "\n",
    "    # Convert metadata list into a DataFrame\n",
    "    return pd.DataFrame(all_metadata)\n",
    "\n",
    "\n",
    "        # if i < num_chunks - 1:  # If not the last iteration\n",
    "        #     for k, v in extracted_metadata.items():\n",
    "        #         if v:  # If the value is not empty\n",
    "        #             existing_values = set([extracted_metadata[k]][0].strip().split(\" \"))  # Convert existing values to a set\n",
    "        #             # existing_values.add(v)  # Add new value (set avoids duplicates)\n",
    "        #             extracted_metadata[k] = \" | \".join(existing_values)  # Join back without duplicates\n",
    "\n",
    "\n",
    "\n",
    "        # # Append metadata for this chunk\n",
    "        # all_metadata.append(extracted_metadata)\n",
    "\n",
    "\n",
    "    #     # Check for duplicates column-wise\n",
    "    #     is_duplicate = False\n",
    "    #     for prev_metadata in all_metadata:\n",
    "    #         for key in extracted_metadata.keys():\n",
    "    #             if key in prev_metadata and extracted_metadata[key] == prev_metadata[key]:  \n",
    "    #                 is_duplicate = True\n",
    "    #                 continue\n",
    "    #         if is_duplicate:\n",
    "    #             break  \n",
    "\n",
    "    #     # If the entire metadata row is not a duplicate, append it\n",
    "    #     if not is_duplicate:\n",
    "    #         all_metadata.append(extracted_metadata)\n",
    "\n",
    "    # Convert metadata list into a DataFrame\n",
    "    # return pd.DataFrame(all_metadata)\n",
    "\n",
    "        # Remove \"%%%%\" values before storing\n",
    "        # extracted_metadata = {k: (v if v != \"%%%\" else \"\") for k, v in extracted_metadata.items()}\n",
    "\n",
    "        # Check for duplicates column-wise\n",
    "    #     cleaned_metadata = {\"Speaker\": speaker}  # Keep speaker info\n",
    "    #     for key, value in extracted_metadata.items():\n",
    "    #         if key == \"Speaker\":\n",
    "    #             continue  # Skip speaker key check\n",
    "    #         is_duplicate = any(value == prev_row.get(key, None) for prev_row in all_metadata)\n",
    "            \n",
    "    #         if not is_duplicate:  # Add only if value is unique\n",
    "    #             cleaned_metadata[key] = value\n",
    "\n",
    "    #         for key in cleaned_metadata.keys():\n",
    "    #             values = [v for v in cleaned_metadata[key].split(\" | \") if v]  # Remove empty values\n",
    "    #             cleaned_metadata[key] = \" | \".join(values) \n",
    "\n",
    "\n",
    "    #     # If there’s any new unique metadata, add it\n",
    "    #     if len(cleaned_metadata) > 1:  \n",
    "    #         all_metadata.append(cleaned_metadata)\n",
    "\n",
    "    # # Convert metadata list into a DataFrame\n",
    "    # return pd.DataFrame(all_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "944efa9a-7293-4fad-8106-d4841c2ac707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing file: file 1.csv\n",
      "Processing chunk 1/21 for Speaker: INT_AH...\n",
      "Processing chunk 2/21 for Speaker: IP_FA...\n",
      "Processing chunk 3/21 for Speaker: INT_AH...\n",
      "Processing chunk 4/21 for Speaker: IP_FA...\n",
      "Processing chunk 5/21 for Speaker: INT_AH...\n",
      "Processing chunk 6/21 for Speaker: IP_FA...\n",
      "Processing chunk 7/21 for Speaker: IP_FA...\n",
      "Processing chunk 8/21 for Speaker: IP_FA...\n",
      "Processing chunk 9/21 for Speaker: IP_FA...\n",
      "Processing chunk 10/21 for Speaker: IP_FA...\n",
      "Processing chunk 11/21 for Speaker: IP_FA...\n",
      "Processing chunk 12/21 for Speaker: IP_FA...\n",
      "Processing chunk 13/21 for Speaker: IP_FA...\n",
      "Processing chunk 14/21 for Speaker: IP_FA...\n",
      "Processing chunk 15/21 for Speaker: INT_AH...\n",
      "Processing chunk 16/21 for Speaker: IP_FA...\n",
      "Processing chunk 17/21 for Speaker: IP_FA...\n",
      "Processing chunk 18/21 for Speaker: IP_FA...\n",
      "Processing chunk 19/21 for Speaker: IP_FA...\n",
      "Processing chunk 20/21 for Speaker: IP_FA...\n",
      "Processing chunk 21/21 for Speaker: INT_AH...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "folder_path = \"Transcripts ADG0001-10\"\n",
    "MODEL = \"llama-3.3-70b-versatile\"\n",
    "\n",
    "# List to store metadata for all files\n",
    "all_metadata = []\n",
    "for filename in os.listdir(folder_path):\n",
    "    file_path = os.path.join(folder_path, filename)\n",
    "    if os.path.isfile(file_path) & filename.endswith(\".csv\"):\n",
    "        print(f\"\\nProcessing file: {filename}\")\n",
    "        input_data = pd.read_csv(file_path, sep=None, engine='python')\n",
    "        speaker_chunks_df = chunk_transcript(input_data)  # Stores speaker-based chunks\n",
    "        final_chunks_df = chunk_by_sentence(speaker_chunks_df)\n",
    "        # print(final_chunks_df)\n",
    "        \n",
    "\n",
    "        # Extract metadata for the chunks\n",
    "        llama_70b_responses = extract_metadata(client,MODEL , final_chunks_df)\n",
    "\n",
    "        # Ensure that the response DataFrame contains metadata columns\n",
    "        if not llama_70b_responses.empty:\n",
    "            # Merge chunk outputs into a single row \n",
    "            merged_metadata = llama_70b_responses.apply(lambda col: ' '.join(col.dropna().astype(str)))\n",
    "            \n",
    "            for column in merged_metadata.index:\n",
    "                unique_values = set([value.strip() for value in merged_metadata[column].strip().split(\",\")])\n",
    "                list_unique_values = list(filter(None, unique_values))\n",
    "                merged_metadata[column] = \" | \".join(list_unique_values)\n",
    "\n",
    "            # Add filename for reference\n",
    "            # merged_metadata[\"Filename\"] = filename  \n",
    "\n",
    "            # Append to list\n",
    "            all_metadata.append(merged_metadata)\n",
    "        else:\n",
    "            print(f\"No metadata extracted from {filename}\")\n",
    "         \n",
    "        time.sleep(0.5)\n",
    "\n",
    "# Convert list of metadata rows into a single DataFrame\n",
    "final_metadata_df = pd.DataFrame(all_metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "54e0e42e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Speaker</th>\n",
       "      <th>NAME</th>\n",
       "      <th>JAHRGANG</th>\n",
       "      <th>ORT</th>\n",
       "      <th>GESCHLECHT</th>\n",
       "      <th>BERUF</th>\n",
       "      <th>VAT_JG</th>\n",
       "      <th>VAT_KONFESSION</th>\n",
       "      <th>VAT_HERKUN</th>\n",
       "      <th>VAT_SCHULE</th>\n",
       "      <th>VAT_AUSBIL</th>\n",
       "      <th>VAT_STAND</th>\n",
       "      <th>VAT_POLOR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INT_AH | IP_FA</td>\n",
       "      <td></td>\n",
       "      <td>1925</td>\n",
       "      <td>Hemer im Sauerland | Gelsenkirchen-Buer</td>\n",
       "      <td>weiblich</td>\n",
       "      <td>Leitung der rollenden Küche | Büroarbeit</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Überseekaufmann | Ingenieurstudium</td>\n",
       "      <td>Kreisorganisationsleiter bei der Arbeitsfront ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Speaker NAME JAHRGANG                                      ORT  \\\n",
       "0  INT_AH | IP_FA          1925  Hemer im Sauerland | Gelsenkirchen-Buer   \n",
       "\n",
       "  GESCHLECHT                                     BERUF VAT_JG VAT_KONFESSION  \\\n",
       "0   weiblich  Leitung der rollenden Küche | Büroarbeit                         \n",
       "\n",
       "  VAT_HERKUN VAT_SCHULE                          VAT_AUSBIL  \\\n",
       "0                        Überseekaufmann | Ingenieurstudium   \n",
       "\n",
       "                                           VAT_STAND VAT_POLOR  \n",
       "0  Kreisorganisationsleiter bei der Arbeitsfront ...            "
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_metadata_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "54dd4129",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Speaker</th>\n",
       "      <th>NAME</th>\n",
       "      <th>JAHRGANG</th>\n",
       "      <th>ORT</th>\n",
       "      <th>GESCHLECHT</th>\n",
       "      <th>BERUF</th>\n",
       "      <th>VAT_JG</th>\n",
       "      <th>VAT_KONFESSION</th>\n",
       "      <th>VAT_HERKUN</th>\n",
       "      <th>VAT_SCHULE</th>\n",
       "      <th>VAT_AUSBIL</th>\n",
       "      <th>VAT_STAND</th>\n",
       "      <th>VAT_POLOR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INT_AH | IP_FA</td>\n",
       "      <td></td>\n",
       "      <td>1925</td>\n",
       "      <td>Hemer im Sauerland</td>\n",
       "      <td>weiblich</td>\n",
       "      <td>Büroarbeit</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Überseekaufmann</td>\n",
       "      <td>Kreisorganisationsleiter bei der Arbeitsfront ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Speaker NAME JAHRGANG                 ORT GESCHLECHT       BERUF  \\\n",
       "0  INT_AH | IP_FA          1925  Hemer im Sauerland   weiblich  Büroarbeit   \n",
       "\n",
       "  VAT_JG VAT_KONFESSION VAT_HERKUN VAT_SCHULE       VAT_AUSBIL  \\\n",
       "0                                              Überseekaufmann   \n",
       "\n",
       "                                           VAT_STAND VAT_POLOR  \n",
       "0  Kreisorganisationsleiter bei der Arbeitsfront ...            "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(all_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c67b7452",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Kreisorganisationsleiter bei der Arbeitsfront | Geschäftsführer | Fabrikant'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_metadata_df['VAT_STAND'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36fb934",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "957141eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'weiblich'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" | \".join([value for value in set(final_metadata_df['GESCHLECHT'].to_list()[0].strip().split(\" \")) if value != \"\" ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4020106",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['weiblich']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[value for value in set(final_metadata_df['GESCHLECHT'].to_list()[0].strip().split(\" \")) if value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3bb53937-9e0b-4fa3-89c9-d6385c8a2d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_metadata_df.to_csv(\"metadata_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775c9947",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
