{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "35d49b97-55a8-40f7-9861-f450ab0cb16b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: To modify pip, please run the following command:\n",
      "C:\\Users\\anavn\\AppData\\Local\\Programs\\Python\\Python312\\python.exe -m pip install -Uqqq pip --progress-bar off\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: groq in c:\\users\\anavn\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.13.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\anavn\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from groq) (4.8.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\anavn\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\anavn\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from groq) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\anavn\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from groq) (2.10.3)\n",
      "Requirement already satisfied: sniffio in c:\\users\\anavn\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from groq) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in c:\\users\\anavn\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from groq) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\anavn\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\anavn\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (2024.12.14)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\anavn\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\anavn\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\anavn\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in c:\\users\\anavn\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (2.27.1)\n",
      "Requirement already satisfied: nltk in c:\\users\\anavn\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\anavn\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\anavn\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\anavn\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\anavn\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\anavn\\appdata\\roaming\\python\\python312\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install -Uqqq pip --progress-bar off\n",
    "!pip install -qqq groq==0.13.0 --progress-bar off\n",
    "!pip install -qqq python-dotenv==1.0.1 --progress-bar off\n",
    "!pip install groq\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "24bf0abf-f7bd-4f31-abe7-9350fce54151",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\anavn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6436c012-5008-435b-89b5-658b701e9855",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_transcript(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \n",
    "    # Ensure 'Transkript' column has no NaN values\n",
    "    data['Transkript'] = data['Transkript'].fillna(\"\").astype(str)\n",
    "\n",
    "    # Chunking logic\n",
    "    chunked_data = []\n",
    "    current_speaker = None\n",
    "    current_text = \"\"\n",
    "    initial_timestamp = \"\"\n",
    "\n",
    "    for index, row in data.iterrows():\n",
    "        speaker = row['Sprecher']\n",
    "        transcript = row['Transkript']\n",
    "        timestamp = row['Timecode'] \n",
    "\n",
    "        if speaker != current_speaker:\n",
    "            # Save previous chunk if exists\n",
    "            if current_speaker is not None:\n",
    "                chunked_data.append({\n",
    "                    'Speaker': current_speaker,\n",
    "                    'Transcript': current_text.strip(),\n",
    "                    'Initial_Timestamp' : initial_timestamp,\n",
    "                    'Current_Timestamp' : timestamp\n",
    "                })\n",
    "            \n",
    "            # Start a new chunk\n",
    "            current_speaker = speaker\n",
    "            current_text = transcript\n",
    "            initial_timestamp = timestamp \n",
    "        else:\n",
    "            # Continue appending to the same speaker's chunk\n",
    "            current_text += \" \" + transcript if isinstance(transcript, str) else \"\"\n",
    "\n",
    "    # Save the last chunk\n",
    "    if current_speaker is not None:\n",
    "        chunked_data.append({\n",
    "            'Speaker': current_speaker,\n",
    "            'Transcript': current_text.strip(),\n",
    "            'Initial_Timestamp' : initial_timestamp,\n",
    "            'Current_Timestamp' : timestamp\n",
    "        })\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    chunked_df = pd.DataFrame(chunked_data)\n",
    "    \n",
    "    return chunked_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cc851cb5-4772-4693-a60f-fe27baf3425b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_by_sentence(chunked_df: pd.DataFrame, min_tokens=256, max_tokens=512) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Further splits chunks by sentence while ensuring each chunk is within a token range.\n",
    "\n",
    "    Args:\n",
    "        chunked_df (pd.DataFrame): Input DataFrame with 'Speaker' and 'Transcript' columns.\n",
    "        min_tokens (int): Minimum number of tokens per chunk.\n",
    "        max_tokens (int): Maximum number of tokens per chunk.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with sentence-based chunked transcripts.\n",
    "    \"\"\"\n",
    "\n",
    "    # Function to count tokens (approximate, assuming 1 word ≈ 1.2 tokens)\n",
    "    def count_tokens(text):\n",
    "        return len(text.split()) * 1.2  # Rough estimate\n",
    "\n",
    "    # Initialize list for final merged chunks\n",
    "    merged_chunks = []\n",
    "    temp_chunk = []\n",
    "    temp_token_count = 0\n",
    "    speaker = None\n",
    "    initial_timestamp = \"\"\n",
    "    final_timestamp = \"\"\n",
    "\n",
    "    # Process each row\n",
    "    for _, row in chunked_df.iterrows():\n",
    "        sentence = row['Transcript']\n",
    "        sentence_tokens = count_tokens(sentence)\n",
    "\n",
    "        # If adding this chunk keeps us within MAX_TOKENS\n",
    "        if temp_token_count + sentence_tokens <= max_tokens:\n",
    "            if not temp_chunk:\n",
    "                speaker = row['Speaker']  # Store speaker only for new chunks\n",
    "                initial_timestamp = row['Initial_Timestamp']\n",
    "            temp_chunk.append(sentence)\n",
    "            temp_token_count += sentence_tokens\n",
    "        else:\n",
    "            # Save the previous chunk before starting a new one\n",
    "            if temp_chunk:\n",
    "                merged_chunks.append({\n",
    "                    'Speaker': speaker,\n",
    "                    'Transcript': \" \".join(temp_chunk),\n",
    "                    'Initial_Timestamp' : initial_timestamp,\n",
    "                    'Current_Timestamp' : row['Current_Timestamp']\n",
    "                })\n",
    "\n",
    "            # Start a new chunk with the current sentence\n",
    "            temp_chunk = [sentence]\n",
    "            temp_token_count = sentence_tokens\n",
    "            speaker = row['Speaker']\n",
    "            initial_timestamp = row['Initial_Timestamp']\n",
    "            final_timestamp = row['Current_Timestamp']\n",
    "\n",
    "    # Save last chunk if any content remains\n",
    "    if temp_chunk:\n",
    "        merged_chunks.append({\n",
    "            'Speaker': speaker,\n",
    "            'Transcript': \" \".join(temp_chunk),\n",
    "            'Initial_Timestamp' : initial_timestamp,\n",
    "            'Current_Timestamp' : final_timestamp\n",
    "        })\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    final_merged_df = pd.DataFrame(merged_chunks)\n",
    "    \n",
    "    return final_merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e744d3c1-7d68-4abc-bfec-e32b54bed9ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Groq client initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "from groq import Groq\n",
    "import pandas as pd \n",
    "\n",
    "\n",
    "# api_key = \"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\"\n",
    "# api_key = \"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\"\n",
    "# api_key = \"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\"\n",
    "# api_key = \"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\"\n",
    "# api_key = \"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\"\n",
    "api_key = \"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\"\n",
    "# Initialize the Groq client\n",
    "client = Groq(api_key=api_key)\n",
    "\n",
    "print(\"Groq client initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed53c67-a80b-4447-b24b-ec542bc7e00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_metadata(client, model_name: str, chunks: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extract metadata from multiple chunks of a German transcript using the specified model.\n",
    "    \n",
    "    Args:\n",
    "        client: The Groq client object.\n",
    "        model_name: The name of the model to use (e.g., 'llama3-8b-8192').\n",
    "        chunks: A DataFrame with 'Speaker' and 'Transcript' columns.\n",
    "    \n",
    "    Returns:\n",
    "        A DataFrame containing the extracted metadata for all chunks.\n",
    "    \"\"\"\n",
    "    all_metadata = []\n",
    "    num_chunks = len(chunks)\n",
    "\n",
    "    for i, row in chunks.iloc[:5].iterrows():  # Iterate over DataFrame rows\n",
    "        speaker = row[\"Speaker\"]\n",
    "        transcript = row[\"Transcript\"]\n",
    "        timestamp = row['Timestamp']\n",
    "\n",
    "        print(f\"Processing chunk {i+1}/{len(chunks)} for Speaker: {speaker}...\")\n",
    "\n",
    "        # Prompt for metadata extraction\n",
    "        prompt = f\"\"\"\n",
    "Extrahieren Sie die folgenden Informationen aus dem Transkript und geben Sie die Antwort auf Deutsch ein. \n",
    "Wenn die Antwort nicht gefunden wurde, geben Sie stattdessen %%% zurück. \n",
    "Fügen Sie nach jedem Wert in Klammern den entsprechenden {timestamp} hinzu. \n",
    "aber nur, wenn der Wert nicht %%% ist und den Wert aus dem Kontext behalten, aber den Zeitstempel nicht hinzufügen.\n",
    "\n",
    "### Persönliche Informationen:\n",
    "NAME: Der vollständige Name der Person (Vor- und Nachname, falls verfügbar).\n",
    "JAHRGANG: Das Geburtsjahr der Person.\n",
    "ORT: Der Geburtsort der Person.\n",
    "GESCHLECHT: Das Geschlecht der Person (z.B. männlich, weiblich, divers).\n",
    "BERUF: Der aktuelle Beruf der Person.\n",
    "\n",
    "### Informationen über Vater und Partner:\n",
    "VAT_JG: Geburtsjahr des Vaters.\n",
    "VAT_KONFESSION: Religion des Vaters.\n",
    "VAT_HERKUN: Herkunft des Vaters.\n",
    "VAT_SCHULE: Schulbildung des Vaters.\n",
    "VAT_AUSBIL: Ausbildung des Vaters.\n",
    "VAT_STAND: Beruflicher Status des Vaters.\n",
    "VAT_POLOR: Politische Orientierung des Vaters.\n",
    "\n",
    "### Informationen zu Ausbildung und Beruf**\n",
    " \n",
    "- **Schulabsch**: Schulbildungsniveau.\n",
    "- **ABGEBROCHE**: Abgebrochene Ausbildung.\n",
    "- **WEITERBILD**: Weiterbildung oder Ausbildung.\n",
    "- **AUSBILDUNG**: Berufsausbildung.\n",
    "- **STAND**: Aktueller beruflicher Status.\n",
    "- **BERUFSWECH**: Berufswechsel (falls zutreffend).\n",
    "- **WANN_WECHS**: Wann der Berufswechsel stattfand.\n",
    "- **AUFABSTIEG**: Karriereaufstieg oder -rückgang.\n",
    "- **BERUFSBEGI**: Berufseinstiegsjahr.\n",
    "- **BERUFSENDE**: Berufsabschlussjahr.\n",
    "- **NICHTERWER**: Nichterwerbstätiger Status (falls zutreffend).\n",
    "- **GRÜNDE**: Gründe für Nichtbeschäftigung.\n",
    "\n",
    "### Informationen zu Familie und Beziehungen**\n",
    " \n",
    "- **FAM_STAND**: Familienstand.\n",
    "- **HEIRAT1JHR, HEIRAT2JHR, HEIRAT3JHR**: Ehejahre (falls zutreffend).\n",
    "- **SCHEID1JHR, SCHEID2JHR**: Scheidungsjahre.\n",
    "- **VERWIT1JHR, VERWIT2JHR**: Witwenjahre.\n",
    "- **KINDERZAHL**: Anzahl der Kinder.\n",
    "- **GEB_JAHR1, GEB_JAHR2, GEB_JAHR_L**: Geburtsjahre der Kinder.\n",
    "\n",
    "Transkript:\n",
    "{transcript}\n",
    "\n",
    "Metadaten:\n",
    "NAME: \n",
    "JAHRGANG: \n",
    "ORT: \n",
    "GESCHLECHT: \n",
    "BERUF: \n",
    "VAT_JG: \n",
    "VAT_KONFESSION: \n",
    "VAT_HERKUN: \n",
    "VAT_SCHULE: \n",
    "VAT_AUSBIL: \n",
    "VAT_STAND: \n",
    "VAT_POLOR:\n",
    "SCHULABSCH:\n",
    "ABGEBROCHE:\n",
    "WEITERBILD:\n",
    "AUSBILDUNG:\n",
    "STAND:\n",
    "WIRTSCHBER:\n",
    "BERUFSWECH:\n",
    "WANN_WECHS:\n",
    "AUFABSTIEG:\n",
    "BERUFSBEGI:\n",
    "BERUFSENDE:\n",
    "NICHTERWER:\n",
    "GRÜNDE:\n",
    "VON_BIS:\n",
    "ARBEITSLOS:\n",
    "VON_BIS_AL:\n",
    "FAM_STAND:\n",
    "HEIRAT1JHR:\n",
    "HEIRAT2JHR:\n",
    "HEIRAT3JHR:\n",
    "SCHEID1JHR:\n",
    "SCHEID2JHR:\n",
    "VERWIT1JHR:\n",
    "VERWIT2JHR:\n",
    "KINDERZAHL:\n",
    "GEB_JAHR1:\n",
    "GEB_JAHR2:\n",
    "GEB_JAHR_L:\n",
    "\n",
    " \n",
    "\n",
    "Fügen Sie keine Präambel ein.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "        # Generate response using the Groq client\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0,\n",
    "        )\n",
    "\n",
    "        # Extract metadata from the response\n",
    "        metadata = response.choices[0].message.content\n",
    "\n",
    "        # Parse metadata into a dictionary\n",
    "        extracted_metadata = {\"Speaker\": speaker}  # Store speaker info\n",
    "        for line in metadata.split(\"\\n\"):\n",
    "            if \":\" in line:\n",
    "                key, value = line.split(\":\", 1)\n",
    "                extracted_metadata[key.strip()] = value.strip()\n",
    "\n",
    "        # Remove \"%%%%\" values before storing\n",
    "        extracted_metadata = {k: (v if v != \"%%%\" else \"\") for k, v in extracted_metadata.items()}\n",
    "\n",
    "        if i <  - 1:  # If not the last iteration\n",
    "            extracted_metadata = {k: v.strip() + \",\" if v else v.strip() for k, v in extracted_metadata.items()}\n",
    "\n",
    "        # Append metadata for this chunk\n",
    "        all_metadata.append(extracted_metadata)\n",
    "\n",
    "    # Convert metadata list into a DataFrame\n",
    "    return pd.DataFrame(all_metadata)\n",
    "\n",
    "\n",
    "        # if i < num_chunks - 1:  # If not the last iteration\n",
    "        #     for k, v in extracted_metadata.items():\n",
    "        #         if v:  # If the value is not empty\n",
    "        #             existing_values = set([extracted_metadata[k]][0].strip().split(\" \"))  # Convert existing values to a set\n",
    "        #             # existing_values.add(v)  # Add new value (set avoids duplicates)\n",
    "        #             extracted_metadata[k] = \" | \".join(existing_values)  # Join back without duplicates\n",
    "\n",
    "\n",
    "\n",
    "        # # Append metadata for this chunk\n",
    "        # all_metadata.append(extracted_metadata)\n",
    "\n",
    "\n",
    "    #     # Check for duplicates column-wise\n",
    "    #     is_duplicate = False\n",
    "    #     for prev_metadata in all_metadata:\n",
    "    #         for key in extracted_metadata.keys():\n",
    "    #             if key in prev_metadata and extracted_metadata[key] == prev_metadata[key]:  \n",
    "    #                 is_duplicate = True\n",
    "    #                 continue\n",
    "    #         if is_duplicate:\n",
    "    #             break  \n",
    "\n",
    "    #     # If the entire metadata row is not a duplicate, append it\n",
    "    #     if not is_duplicate:\n",
    "    #         all_metadata.append(extracted_metadata)\n",
    "\n",
    "    # Convert metadata list into a DataFrame\n",
    "    # return pd.DataFrame(all_metadata)\n",
    "\n",
    "        # Remove \"%%%%\" values before storing\n",
    "        # extracted_metadata = {k: (v if v != \"%%%\" else \"\") for k, v in extracted_metadata.items()}\n",
    "\n",
    "        # Check for duplicates column-wise\n",
    "    #     cleaned_metadata = {\"Speaker\": speaker}  # Keep speaker info\n",
    "    #     for key, value in extracted_metadata.items():\n",
    "    #         if key == \"Speaker\":\n",
    "    #             continue  # Skip speaker key check\n",
    "    #         is_duplicate = any(value == prev_row.get(key, None) for prev_row in all_metadata)\n",
    "            \n",
    "    #         if not is_duplicate:  # Add only if value is unique\n",
    "    #             cleaned_metadata[key] = value\n",
    "\n",
    "    #         for key in cleaned_metadata.keys():\n",
    "    #             values = [v for v in cleaned_metadata[key].split(\" | \") if v]  # Remove empty values\n",
    "    #             cleaned_metadata[key] = \" | \".join(values) \n",
    "\n",
    "\n",
    "    #     # If there’s any new unique metadata, add it\n",
    "    #     if len(cleaned_metadata) > 1:  \n",
    "    #         all_metadata.append(cleaned_metadata)\n",
    "\n",
    "    # # Convert metadata list into a DataFrame\n",
    "    # return pd.DataFrame(all_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "944efa9a-7293-4fad-8106-d4841c2ac707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing file: adg0007_er_2024_10_31.csv\n",
      "Processing chunk 1/44 for Speaker: INT_AVP...\n",
      "Processing chunk 2/44 for Speaker: IP_MB...\n",
      "Processing chunk 3/44 for Speaker: IP_MB...\n",
      "Processing chunk 4/44 for Speaker: INT_AVP...\n",
      "Processing chunk 5/44 for Speaker: IP_MB...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "folder_path = \"Transcripts ADG0001-10\"\n",
    "MODEL = \"llama-3.3-70b-versatile\"\n",
    "\n",
    "# List to store metadata for all files\n",
    "all_metadata = []\n",
    "for filename in os.listdir(folder_path):\n",
    "    file_path = os.path.join(folder_path, filename)\n",
    "    if os.path.isfile(file_path) & filename.endswith(\".csv\"):\n",
    "        print(f\"\\nProcessing file: {filename}\")\n",
    "        input_data = pd.read_csv(file_path, sep=None, engine='python')\n",
    "        speaker_chunks_df = chunk_transcript(input_data)  # Stores speaker-based chunks\n",
    "        final_chunks_df = chunk_by_sentence(speaker_chunks_df)\n",
    "        final_chunks_df['Timestamp'] = final_chunks_df['Initial_Timestamp'] + \" - \" + final_chunks_df['Current_Timestamp'] \n",
    "        final_chunks_df.drop(columns=['Initial_Timestamp', \"Current_Timestamp\"], inplace=True)\n",
    "        # print(final_chunks_df)\n",
    "        \n",
    "\n",
    "        # Extract metadata for the chunks\n",
    "        llama_70b_responses = extract_metadata(client,MODEL , final_chunks_df)\n",
    "\n",
    "        # Ensure that the response DataFrame contains metadata columns\n",
    "        if not llama_70b_responses.empty:\n",
    "            # Merge chunk outputs into a single row \n",
    "            merged_metadata = llama_70b_responses.apply(lambda col: ' '.join(col.dropna().astype(str)))\n",
    "            \n",
    "            for column in merged_metadata.index:\n",
    "                unique_values = set([value.strip() for value in merged_metadata[column].strip().split(\",\")])\n",
    "                list_unique_values = list(filter(None, unique_values))\n",
    "                merged_metadata[column] = \" | \".join(list_unique_values)\n",
    "\n",
    "            # Add filename for reference\n",
    "            # merged_metadata[\"Filename\"] = filename  \n",
    "\n",
    "            # Append to list\n",
    "            all_metadata.append(merged_metadata)\n",
    "        else:\n",
    "            print(f\"No metadata extracted from {filename}\")\n",
    "         \n",
    "        time.sleep(0.5)\n",
    "\n",
    "# Convert list of metadata rows into a single DataFrame\n",
    "final_metadata_df = pd.DataFrame(all_metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7f7933aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Speaker</th>\n",
       "      <th>NAME</th>\n",
       "      <th>JAHRGANG</th>\n",
       "      <th>ORT</th>\n",
       "      <th>GESCHLECHT</th>\n",
       "      <th>BERUF</th>\n",
       "      <th>VAT_JG</th>\n",
       "      <th>VAT_KONFESSION</th>\n",
       "      <th>VAT_HERKUN</th>\n",
       "      <th>VAT_SCHULE</th>\n",
       "      <th>...</th>\n",
       "      <th>HEIRAT2JHR</th>\n",
       "      <th>HEIRAT3JHR</th>\n",
       "      <th>SCHEID1JHR</th>\n",
       "      <th>SCHEID2JHR</th>\n",
       "      <th>VERWIT1JHR</th>\n",
       "      <th>VERWIT2JHR</th>\n",
       "      <th>KINDERZAHL</th>\n",
       "      <th>GEB_JAHR1</th>\n",
       "      <th>GEB_JAHR2</th>\n",
       "      <th>GEB_JAHR_L</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IP_MB | INT_AVP</td>\n",
       "      <td>Frau Bessel (00:00:07.13 - 00:00:10.15)</td>\n",
       "      <td></td>\n",
       "      <td>Rüttenscheid (00:03:59.03 - 00:07:59.04)</td>\n",
       "      <td>weiblich (00:00:07.13 - 00:00:10.15) | weiblich</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Speaker                                     NAME JAHRGANG  \\\n",
       "0  IP_MB | INT_AVP  Frau Bessel (00:00:07.13 - 00:00:10.15)            \n",
       "\n",
       "                                        ORT  \\\n",
       "0  Rüttenscheid (00:03:59.03 - 00:07:59.04)   \n",
       "\n",
       "                                        GESCHLECHT BERUF VAT_JG  \\\n",
       "0  weiblich (00:00:07.13 - 00:00:10.15) | weiblich                \n",
       "\n",
       "  VAT_KONFESSION VAT_HERKUN VAT_SCHULE  ... HEIRAT2JHR HEIRAT3JHR SCHEID1JHR  \\\n",
       "0                                       ...                                    \n",
       "\n",
       "  SCHEID2JHR VERWIT1JHR VERWIT2JHR KINDERZAHL GEB_JAHR1 GEB_JAHR2 GEB_JAHR_L  \n",
       "0                                                                             \n",
       "\n",
       "[1 rows x 37 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_metadata_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d9cf552",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Speaker</th>\n",
       "      <th>NAME</th>\n",
       "      <th>JAHRGANG</th>\n",
       "      <th>ORT</th>\n",
       "      <th>GESCHLECHT</th>\n",
       "      <th>BERUF</th>\n",
       "      <th>VAT_JG</th>\n",
       "      <th>VAT_KONFESSION</th>\n",
       "      <th>VAT_HERKUN</th>\n",
       "      <th>VAT_SCHULE</th>\n",
       "      <th>VAT_AUSBIL</th>\n",
       "      <th>VAT_STAND</th>\n",
       "      <th>VAT_POLOR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IP_FA | INT_AH</td>\n",
       "      <td></td>\n",
       "      <td>1925 (00:00:06.00 - 00:04:51.00)</td>\n",
       "      <td>Dortmund (00:30:21.00 - 00:37:48.00) | Gelsenk...</td>\n",
       "      <td>weiblich</td>\n",
       "      <td>Büroarbeit (01:08:53.00) | Büroarbeit (00:11:1...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Überseekaufmann (00:00:06.00 - 00:04:51.00) | ...</td>\n",
       "      <td>Geschäftsführer (00:02:20.00 - 00:02:30.00) | ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Speaker NAME                          JAHRGANG  \\\n",
       "0  IP_FA | INT_AH       1925 (00:00:06.00 - 00:04:51.00)   \n",
       "\n",
       "                                                 ORT GESCHLECHT  \\\n",
       "0  Dortmund (00:30:21.00 - 00:37:48.00) | Gelsenk...   weiblich   \n",
       "\n",
       "                                               BERUF VAT_JG VAT_KONFESSION  \\\n",
       "0  Büroarbeit (01:08:53.00) | Büroarbeit (00:11:1...                         \n",
       "\n",
       "  VAT_HERKUN VAT_SCHULE                                         VAT_AUSBIL  \\\n",
       "0                        Überseekaufmann (00:00:06.00 - 00:04:51.00) | ...   \n",
       "\n",
       "                                           VAT_STAND VAT_POLOR  \n",
       "0  Geschäftsführer (00:02:20.00 - 00:02:30.00) | ...            "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_metadata_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb53937-9e0b-4fa3-89c9-d6385c8a2d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_metadata_df.to_csv(\"metadata_results.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
